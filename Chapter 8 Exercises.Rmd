---
title: "Chapter 8 Exercises"
author: "Eugene Yan"
date: "8 March 2015"
output: html_document
---

### Qn 1
```{r}
par(xpd=NA)
plot(NA, NA, type="n", xlim=c(0,100), ylim=c(0,100), xlab="X", ylab="Y")
# t1: x = 40; (40, 0) (40, 100)
lines(x=c(40,40),y=c(0,100))
text(x=40, y=108, labels=c("t1"), col="red")
# t2: y = 75; (0, 75) (40, 75)
lines(x=c(0,40), y=c(75,75))
text(x=-8, y=75, labels=c("t2"), col="red")
# t3: x = 75; (75,0) (75, 100)
lines(x=c(75,75),y=c(0,100))
text(x=75, y=108, labels=c("t3"), col="red")
# t4: x = 20; (20,0) (20, 75)
lines(x=c(20,20),y=c(0,75))
text(x=20, y=80, labels=c("t4"), col="red")
# t5: y=25; (75,25) (100,25)
lines(x=c(75,100),y=c(25,25))
text(x=70, y=25, labels=c("t5"), col="red")

text(x=(40+75)/2, y=50, labels=c("R1"))
text(x=20, y=(100+75)/2, labels=c("R2"))
text(x=(75+100)/2, y=(100+25)/2, labels=c("R3"))
text(x=(75+100)/2, y=25/2, labels=c("R4"))
text(x=30, y=75/2, labels=c("R5"))
text(x=10, y=75/2, labels=c("R6"))
```
```
        [  X<40 ] 
        |       |
    [Y<75]    [X<75]
    |   |     |    |
 [X<20] R2    R1   [Y<25]
 |    |            |    |
 R6   R5           R4   R3
```
### Qn 2

Based on Algorithm 8.2, the first stump will consist of a split on a single variable. By induction, the residuals of that first fit will result in a second stump fit to a another distinct, single variable. (* This is my intuition, not sure if my proof is rigorous enough to support that claim).

$f(X) = \sum_{j=1}^{p} f_j(X_j)$

0) $\hat{f}(x) = 0, r_i = y_i$

1) a) $\hat{f}^1(x) = \beta_{1_1} I(X_1 < t_1) + \beta_{0_1}$

1) b) $\hat{f}(x) = \lambda\hat{f}^1(x)$

1) c) $r_i = y_i - \lambda\hat{f}^1(x_i)$

To maximize the fit to the residuals, another distinct stump must be fit in the next and subsequent iterations will each fit $X_j$-distinct stumps. The following is the jth iteration, where $b=j$:

j) a) $\hat{f}^j(x) = \beta_{1_j} I(X_j < t_j) + \beta_{0_j}$

j) b) $\hat{f}(x) = \lambda\hat{f}^1(X_1) + \dots + \hat{f}^j(X_j) + \dots + \hat{f}^{p-1}(X_{p-1}) + \hat{f}^p(X_p)$

Since each iteration's fit is a distinct variable stump, there are only $p$ fits based on "j) b)".

$$f(X) = \sum_{j=1}^{p} f_j(X_j)$$

### Qn 3

```{r}
p = seq(0, 1, .01)
gini = p * (1-p) * 2
entropy = - (p * log(p) + (1-p) * log(1-p))
class.err = 1 - pmax(p, 1-p)
matplot(p, cbind(gini, entropy, class.err), col=c("red", "green", "blue"))
legend('bottom', legend = c('Gini', 'Entrophy', 'Class Error'), col = c('red', 'green', 'blue'), lty = 1, lwd = 2, cex = 0.8)
```
For all three measurements, they take a value close to zero the more pure p is (either p closer to 0 or p closer to 1).  However, Gini is more sensitive than classification error (as it is mostly above the classification error curve), while Entrophy is more sensitive than Gini.  

### Qn 4.a
```
              [X1 < 1]
              |      |
       [X2 < 1]      5
       |      |
[X1 < 0]      15
|      |
3      [X2<0]
       |    |
      10    0
```

### Qn 4.b
```{r}
par(xpd=NA)
plot(NA, NA, type="n", xlim=c(-2,2), ylim=c(-3,3), xlab="X1", ylab="X2")
# X2 < 1
lines(x=c(-2,2), y=c(1,1))
# X1 < 1 with X2 < 1
lines(x=c(1,1), y=c(-3,1))
text(x=(-2+1)/2, y=-1, labels=c(-1.80))
text(x=1.5, y=-1, labels=c(0.63))
# X2 < 2 with X2 >= 1
lines(x=c(-2,2), y=c(2,2))
text(x=0, y=2.5, labels=c(2.49))
# X1 < 0 with X2<2 and X2>=1
lines(x=c(0,0), y=c(1,2))
text(x=-1, y=1.5, labels=c(-1.06))
text(x=1, y=1.5, labels=c(0.21))
```

### Qn 5
By majority vote, 'red' would have 6 votes and 'not red' (i.e., green) 4 votes.  Thus, it would classify as green.

By averaging, the probability of red would be 0.45.  Thus, it would classify as green.

